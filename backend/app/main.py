from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import os
import json
from pathlib import Path
from dotenv import load_dotenv  # type: ignore
import uuid
import time

# === LangChain ê´€ë ¨ ì„í¬íŠ¸ ===
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain.schema import BaseOutputParser, StrOutputParser
from langchain.chains import LLMChain, SequentialChain
from langchain.agents import AgentExecutor, create_structured_chat_agent
from langchain.tools import BaseTool
from langchain.schema.runnable import RunnablePassthrough
from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import MessagesPlaceholder
from langchain.memory import ConversationBufferMemory
from langchain.schema.messages import AIMessage, HumanMessage

# Google Generative AI í´ë¼ì´ì–¸íŠ¸ (ì´ë¯¸ì§€ ìƒì„±ìš©)
import google.generativeai as genai

# === ë°ì´í„° ì²˜ë¦¬ ë° ìœ í‹¸ë¦¬í‹° ===
from dataclasses import dataclass
import numpy as np
from collections import Counter

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ í™•ì¸
load_dotenv()

# LangChain ì„¤ì •
LLM_MODEL = os.getenv("GEMINI_MODEL", "gemini-1.5-flash")  # ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ëª¨ë¸
LLM_TEMP = float(os.getenv("GEMINI_TEMPERATURE", "0.2"))
EMBED_MODEL = "models/embedding-001"  # ì˜¬ë°”ë¥¸ Gemini embedding ëª¨ë¸ëª…

# Gemini API í‚¤ í™•ì¸
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    print("âš ï¸  WARNING: GEMINI_API_KEY not found in environment variables!")
    print("   Please set GEMINI_API_KEY in your .env file or environment")
else:
    print(f"âœ… Gemini API Key loaded: {GEMINI_API_KEY[:8]}...")
    print(f"   Model: {LLM_MODEL}, Temperature: {LLM_TEMP}")

# LangChain LLM ë° Embeddings ì´ˆê¸°í™”
try:
    llm = ChatGoogleGenerativeAI(
        model=LLM_MODEL,
        temperature=LLM_TEMP,
        google_api_key=GEMINI_API_KEY
    )
    embeddings = GoogleGenerativeAIEmbeddings(
        model=EMBED_MODEL,
        google_api_key=GEMINI_API_KEY
    )
    
    # Google Generative AI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    genai.configure(api_key=GEMINI_API_KEY)
    
    print("âœ… LangChain Gemini components initialized successfully")
except Exception as e:
    print(f"âŒ Failed to initialize LangChain Gemini components: {e}")
    llm = None
    embeddings = None


# -----------------------------
# ì €ì¥ ê²½ë¡œ/ìœ í‹¸
# -----------------------------
DATA_DIR = Path(__file__).resolve().parents[1] / "data"
DATA_DIR.mkdir(parents=True, exist_ok=True)
RESULTS_PATH = DATA_DIR / "abtest_results.jsonl"

def append_jsonl(path: Path, row: dict) -> None:
    with path.open("a", encoding="utf-8") as f:
        f.write(json.dumps(row, ensure_ascii=False) + "\n")

def update_user_choice_inplace(log_id: str, user_final_text: str) -> bool:
    """
    JSONLì—ì„œ í•´ë‹¹ log_idì˜ 'ë³¸ ë ˆì½”ë“œ(ì˜ˆì¸¡ ë ˆì½”ë“œ)'ë§Œ ì°¾ì•„ user_final_textë¥¼ ê°±ì‹ .
    - ê¸°ì¡´ì— appendí•´ë‘” user_choice_update ì´ë²¤íŠ¸ ë¼ì¸ì€ ì •ë¦¬(ì‚­ì œ).
    - ëŒ€ìƒì´ ì—†ìœ¼ë©´ False.
    """
    src = RESULTS_PATH
    tmp = RESULTS_PATH.with_suffix(".jsonl.tmp")

    if not src.exists():
        return False

    updated = False
    with src.open("r", encoding="utf-8") as fin, tmp.open("w", encoding="utf-8") as fout:
        for line in fin:
            try:
                obj = json.loads(line)
            except Exception:
                # ê¹¨ì§„ ë¼ì¸ì€ ìœ ì§€
                fout.write(line)
                continue

            # ê³¼ê±° ì´ë²¤íŠ¸ ë¼ì¸ì€ ì •ë¦¬
            if obj.get("type") == "user_choice_update" and obj.get("log_id") == log_id:
                continue

            # ë³¸ ë ˆì½”ë“œ: log_id ì¼ì¹˜ + ì˜ˆì¸¡ í•„ë“œ ì¡´ì¬
            if (not updated) and obj.get("log_id") == log_id and "pred_ctr_a" in obj and "pred_ctr_b" in obj:
                obj["user_final_text"] = user_final_text
                updated = True

            fout.write(json.dumps(obj, ensure_ascii=False) + "\n")

    os.replace(tmp, src)
    return updated

# -----------------------------
# ì €ì¥ ë ˆì½”ë“œ ìŠ¤í‚¤ë§ˆ
# -----------------------------
class ABTestStoredResult(BaseModel):
    # ìš´ì˜ í¸ì˜
    log_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: float = Field(default_factory=lambda: time.time())

    # ì„¸ê·¸ë¨¼íŠ¸/ì…ë ¥
    age_groups: List[str] = []
    genders: List[str] = []
    interests: str
    category: str
    marketing_a: str
    marketing_b: str

    # ì˜ˆì¸¡ê°’
    pred_ctr_a: float
    pred_ctr_b: float
    pred_ctr_c: float

    # AI ì‚°ì¶œë¬¼
    ai_generated_text: str       # ì œ3ì˜ ë¬¸êµ¬(= AIê°€ ìƒˆë¡œ ë§Œë“  ì¹´í”¼)
    ai_top_ctr_choice: str       # "A", "B", or "C" (ê°€ì¥ ë†’ì€ ì˜ˆì¸¡)

    # ì‚¬ìš©ì ìµœì¢… ì„ íƒ(ì˜ˆì¸¡ ì‹œì ì—” None)
    user_final_text: Optional[str] = None

# === LangChain í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ ===
PERSONA_ANALYSIS_TEMPLATE = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ 30ë…„ ê²½ë ¥ì˜ í˜ë¥´ì†Œë‚˜ ë§ˆì¼€íŒ… ë¶„ì„ê°€ì…ë‹ˆë‹¤. 
ê° í˜ë¥´ì†Œë‚˜ ì…ì¥ì—ì„œ ë§ˆì¼€íŒ… ë¬¸êµ¬ì— ëŒ€í•œ ì‹¬ë„ ê¹Šì€ ë¶„ì„ì„ ì œê³µí•˜ë˜, 
ê° ë¶„ì„ ë‚´ìš©ì„ 300ì ì´ìƒì˜ ìƒì„¸í•œ ë³´ê³ ì„œ í˜•íƒœë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

STRICT JSON í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•´ì£¼ì„¸ìš”."""),
    ("human", """[ì…ë ¥ íƒ€ê²Ÿ]
- category: {category}
- ages: {ages}
- genders: {genders}
- interests: {interests}

[ì¹´í”¼]
- A: {marketing_a}
- B: {marketing_b}

[í˜ë¥´ì†Œë‚˜]
{personas}

[ë°˜í™˜ í˜•ì‹: STRICT JSON with detailed analysis]
{{
  "results": [
    {{
      "persona_id": "p1",
      "score_a": 1,
      "score_b": 5,
      "detailed_analysis": {{
        "a_analysis": "Aì•ˆì— ëŒ€í•œ 300ì ì´ìƒì˜ ìƒì„¸í•œ ë¶„ì„ ë‚´ìš©. í˜ë¥´ì†Œë‚˜ íŠ¹ì„±ê³¼ì˜ ì—°ê´€ì„±, í´ë¦­ ì˜í–¥ì— ë¯¸ì¹˜ëŠ” ìš”ì†Œë“¤ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ìš”.",
        "b_analysis": "Bì•ˆì— ëŒ€í•œ 300ì ì´ìƒì˜ ìƒì„¸í•œ ë¶„ì„ ë‚´ìš©. í˜ë¥´ì†Œë‚˜ íŠ¹ì„±ê³¼ì˜ ì—°ê´€ì„±, í´ë¦­ ì˜í–¥ì— ë¯¸ì¹˜ëŠ” ìš”ì†Œë“¤ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ìš”.",
        "overall_evaluation": "ì „ì²´ì ì¸ ë§ˆì¼€íŒ… íš¨ê³¼ì™€ ê°œì„  ë°©í–¥, í˜ë¥´ì†Œë‚˜ë³„ ë§ì¶¤ ì „ëµì— ëŒ€í•œ 300ì ì´ìƒì˜ ì¢…í•© í‰ê°€ë¥¼ ì œê³µí•´ìš”."
      }},
      "reasons": ["í•µì‹¬ìš”ì¸1", "í•µì‹¬ìš”ì¸2", "í•µì‹¬ìš”ì¸3"]
    }}
  ],
  "winner_reason_keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"]
}}

ê° ë¶„ì„ í•­ëª©ì„ 300ì ì´ìƒì˜ ì‹¬ë„ ê¹Šì€ ë‚´ìš©ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ëª¨ë“  ë¬¸ì¥ì€ ë°˜ë“œì‹œ '-ìš”'ë¡œ ëë‚˜ì•¼ í•´ìš”.""")
])

COPY_ANALYSIS_TEMPLATE = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ 30ë…„ ê²½ë ¥ì˜ ë§ˆì¼€íŒ… ë¶„ì„ê°€ì…ë‹ˆë‹¤. 
ë‘ ê´‘ê³  ë¬¸êµ¬ A/Bì˜ CTRì„ ì˜ˆì¸¡í•˜ê³  ê°ê°ì— ëŒ€í•´ 300ì ì´ìƒì˜ ì‹¬ë„ ê¹Šì€ ë¶„ì„ì„ ì œê³µí•´ìš”. 
ì‹œì¥ì§€ì‹, ëª…í™•ì„±, ê´€ë ¨ì„±, êµ¬ì²´ì„±, í›„í‚¹(ìˆ˜ì¹˜, ê¸´ë°•ê°, ì‚¬íšŒì  ì¦ê±°)ì„ ê·¼ê±°ë¡œ í‰ê°€í•´ìš”.

í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ë˜, ëª¨ë“  ë¬¸ì¥ì„ '-ìš”'ë¡œ ëë‚˜ê²Œ ì‘ì„±í•´ìš”."""),
    ("human", """íƒ€ê²Ÿ: {audience}
ì œí’ˆ/ì¹´í…Œê³ ë¦¬: {category}

Aì•ˆ: {marketing_a}
Bì•ˆ: {marketing_b}

ë°˜ë“œì‹œ ë‹¤ìŒ í‚¤ë¥¼ ê°€ì§„ JSONë§Œ ì¶œë ¥í•´ìš”: ctr_a, ctr_b, analysis_a, analysis_b, ai_suggestion.
ê·œì¹™:
- ctr_a/ctr_bëŠ” 0~1 ì‚¬ì´ ì‹¤ìˆ˜ì˜ˆìš” (ì˜ˆ: 0.123ì€ 12.3%).
- analysis_a/analysis_bëŠ” ê°ê° 300ì ì´ìƒì˜ ì‹¬ë„ ê¹Šì€ ë¶„ì„ì´ í¬í•¨ëœ ë¬¸ì¥í˜• í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ˆìš”.
- ai_suggestionì€ ì´ íƒ€ê²Ÿì— ë§ì¶˜ í•œ ì¤„ ë¬¸êµ¬ì˜ˆìš”.
- ëª¨ë“  ë¬¸ì¥ì€ ë°˜ë“œì‹œ '-ìš”'ë¡œ ëë‚˜ì•¼ í•´ìš”.

ê° ë¶„ì„ì€ ë‹¤ìŒ ìš”ì†Œë“¤ì„ í¬í•¨í•˜ì—¬ 300ì ì´ìƒìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:
- ë§ˆì¼€íŒ… ë¬¸êµ¬ì˜ ê°•ì ê³¼ ì•½ì  ë¶„ì„
- íƒ€ê²Ÿ ì˜¤ë””ì–¸ìŠ¤ì™€ì˜ ì—°ê´€ì„± í‰ê°€
- CTR ì˜ˆì¸¡ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” í•µì‹¬ ìš”ì†Œ ë¶„ì„
- ê°œì„  ë°©í–¥ê³¼ ìµœì í™” ì „ëµ ì œì‹œ""")
])

THIRD_COPY_TEMPLATE = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ 30ë…„ ê²½ë ¥ì˜ ë§ˆì¼€íŒ… ì¹´í”¼ë¼ì´í„°ì´ì ë¶„ì„ê°€ì…ë‹ˆë‹¤. 
ì•„ë˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” í•œêµ­ì–´ ë§ˆì¼€íŒ… í—¤ë“œë¼ì¸ì„ 1ê°œ ìƒì„±í•˜ë˜, 
ìƒì„± ê³¼ì •ê³¼ ê·¼ê±°ë¥¼ 300ì ì´ìƒì˜ ìƒì„¸í•œ ë¶„ì„ê³¼ í•¨ê»˜ ì œê³µí•´ì£¼ì„¸ìš”. 
STRICT JSONìœ¼ë¡œë§Œ ë°˜í™˜í•©ë‹ˆë‹¤. ëª¨ë“  ë¬¸ì¥ì€ ë°˜ë“œì‹œ '-ìš”'ë¡œ ëë‚˜ì•¼ í•´ìš”."""),
    ("human", """[ëª©í‘œ]
- ìŠ¹ì ì¹´í”¼({winner})ì˜ ê°•ì ì„ ë°˜ì˜í•´ ë” ë†’ì€ CTRì´ ì˜ˆìƒë˜ëŠ” ë¬¸êµ¬ 1ê°œ

[ìŠ¹ì ìš”ì¸ í‚¤ì›Œë“œ]
{winner_keywords}

[ì œì•½]
- 35ìì—ì„œ 45ì ì‚¬ì´
- CTA ë°˜ë“œì‹œ 1ê°œ í¬í•¨(ì˜ˆ: {cta_examples})
- ê¸ˆì¹™ì–´ í¬í•¨ ê¸ˆì§€: {forbidden_words}
- ê³¼ì¥/í—ˆìœ„ ë¶ˆê°€, ëª…í™•í•˜ë©´ì„œ ì°½ì˜ì„±ìˆê²Œ

[ë°˜í™˜ í˜•ì‹]
{{
  "text": "ìµœì¢… ë¬¸êµ¬",
  "detailed_analysis": {{
    "creation_process": "300ì ì´ìƒì˜ ë¬¸êµ¬ ìƒì„± ê³¼ì •ê³¼ ì „ëµì  ì‚¬ê³  ê³¼ì •ì„ ìƒì„¸íˆ ì„¤ëª…í•´ìš”.",
    "winner_integration": "300ì ì´ìƒì˜ ìŠ¹ì ì¹´í”¼ ê°•ì  í†µí•© ë°©ë²•ê³¼ íš¨ê³¼ ë¶„ì„ì„ ì œê³µí•´ìš”.",
    "ctr_optimization": "300ì ì´ìƒì˜ CTR í–¥ìƒì„ ìœ„í•œ í•µì‹¬ ìš”ì†Œì™€ ê°œì„  ë°©í–¥ ë¶„ì„ì„ ì œì‹œí•´ìš”.",
    "target_audience_consideration": "300ì ì´ìƒì˜ íƒ€ê²Ÿ ì˜¤ë””ì–¸ìŠ¤ íŠ¹ì„±ì„ ë°˜ì˜í•œ ë¬¸êµ¬ ì„¤ê³„ ê·¼ê±°ë¥¼ ì„¤ëª…í•´ìš”."
  }}
}}

ê° ë¶„ì„ í•­ëª©ì„ 300ì ì´ìƒì˜ ì‹¬ë„ ê¹Šì€ ë‚´ìš©ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ëª¨ë“  ë¬¸ì¥ì€ ë°˜ë“œì‹œ '-ìš”'ë¡œ ëë‚˜ì•¼ í•´ìš”.""")
])

C_ANALYSIS_TEMPLATE = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ 30ë…„ ê²½ë ¥ì˜ ë§ˆì¼€íŒ… ë¶„ì„ê°€ì…ë‹ˆë‹¤. 
AIê°€ ìƒì„±í•œ ë§ˆì¼€íŒ… ë¬¸êµ¬ì— ëŒ€í•´ 300ì ì´ìƒì˜ ì‹¬ë„ ê¹Šì€ ë¶„ì„ì„ ì œê³µí•´ìš”.

í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ë˜, ëª¨ë“  ë¬¸ì¥ì„ '-ìš”'ë¡œ ëë‚˜ê²Œ ì‘ì„±í•´ìš”."""),
    ("human", """íƒ€ê²Ÿ: {audience}
ì œí’ˆ/ì¹´í…Œê³ ë¦¬: {category}

AI ìƒì„± ë¬¸êµ¬: {ai_text}

ë‹¤ìŒ ìš”ì†Œë“¤ì„ í¬í•¨í•˜ì—¬ 300ì ì´ìƒìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:
- AI ìƒì„± ë¬¸êµ¬ì˜ ê°•ì ê³¼ ì°¨ë³„í™” ìš”ì†Œ ë¶„ì„
- Aì•ˆ, Bì•ˆ ëŒ€ë¹„ ê°œì„ ëœ ì ê³¼ ìš°ìˆ˜ì„± í‰ê°€
- íƒ€ê²Ÿ ì˜¤ë””ì–¸ìŠ¤ì™€ì˜ ì—°ê´€ì„± ë° ë§¤ë ¥ë„ ë¶„ì„
- CTR ì˜ˆì¸¡ì´ ë†’ì€ ì´ìœ ì™€ í•µì‹¬ ì„±ê³µ ìš”ì¸ ë¶„ì„
- ì‹¤ì œ ë§ˆì¼€íŒ…ì—ì„œì˜ í™œìš© ê°€ëŠ¥ì„±ê³¼ íš¨ê³¼ì„± í‰ê°€

JSON í˜•ì‹ìœ¼ë¡œ analysis_c í‚¤ì— ë¶„ì„ ë‚´ìš©ì„ ë‹´ì•„ì£¼ì„¸ìš”.""")
])

# === í˜ë¥´ì†Œë‚˜ ì‹œë“œ ===
@dataclass
class Persona:
    id: str
    name: str
    weight: float
    categories: list[str]
    ages: list[str]       # "10s","20s","30s","40s","50s"
    genders: list[str]    # "male","female"
    interests: list[str]
    notes: str = ""

PERSONAS: list[Persona] = [
    Persona("p1","ë·°í‹°/í™”ì¥í’ˆ",1.2,["beauty","cosmetics","skincare"],["20s"],["female"],["ìƒí™œ","ë…¸í•˜ìš°","ì‡¼í•‘"]),
    Persona("p2","ê²Œì„",1.0,["gaming","electronics","entertainment"],["20s"],["male"],["ì·¨ë¯¸","ì—¬ê°€","ì—¬í–‰"]),
    Persona("p3","íŒ¨ì…˜/ì¡í™”",0.9,["fashion","accessories","lifestyle"],["30s"],["female"],["ìƒí™œ","ë…¸í•˜ìš°","ì‡¼í•‘"]),
    Persona("p4","ë¶€ë™ì‚°/ì¬í…Œí¬",1.1,["real_estate","investment","finance"],["30s"],["male"],["ì§€ì‹", "ë™í–¥"]),
    Persona("p5","ì—¬í–‰/ìˆ™ë°•/í•­ê³µ",1.0,["travel","accommodation","aviation"],["40s"],["female"],["ì·¨ë¯¸","ì—¬ê°€","ì—¬í–‰"]),
    Persona("p6","ìŠ¤í¬ì¸ /ë ˆì €",1.0,["sports","outdoor","leisure"],["40s"],["male"],["ì·¨ë¯¸","ì—¬ê°€","ì—¬í–‰"]),
    Persona("p7","ì‹ìŒë£Œ/ìš”ë¦¬",1.0,["food","beverage","cooking"],["50s"],["female"],["ìƒí™œ","ë…¸í•˜ìš°","ì‡¼í•‘"]),
    Persona("p8","ì •ì¹˜/ì‚¬íšŒ",1.0,["politics","social_issues","news"],["50s"],["male"],["ì§€ì‹", "ë™í–¥"]),
]

# === LangChain ê¸°ë°˜ ì„ë² ë”© & ìœ ì‚¬ë„ ===
def _embed_text(text: str) -> np.ndarray:
    if not embeddings:
        raise Exception("LangChain embeddings not initialized")
    try:
        embedding = embeddings.embed_query(text[:8000])
        return np.array(embedding, dtype=np.float32)
    except Exception as e:
        print(f"Embedding failed: {e}")
        # Fallback: ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ ë²¡í„° ìƒì„±
        import hashlib
        hash_obj = hashlib.md5(text.encode())
        return np.array([int(hash_obj.hexdigest()[:8], 16) % 1000 / 1000.0 for _ in range(1536)], dtype=np.float32)

def _cosine(a: np.ndarray, b: np.ndarray) -> float:
    da, db = np.linalg.norm(a), np.linalg.norm(b)
    if da == 0 or db == 0: return 0.0
    return float(np.dot(a, b) / (da * db))

def _feature_sentence(category:str, ages:list[str], genders:list[str], interests:str, A:str, B:str)->str:
    return (f"category={category}; ages={','.join(ages or [])}; genders={','.join(genders or [])}; "
            f"interests={interests}; A={A}; B={B}")

# === í˜ë¥´ì†Œë‚˜ ìƒ˜í”Œë§ ===
def _overlap_ratio(a:set[str], b:set[str])->float:
    if not a or not b: return 0.0
    return len(a & b) / max(1, len(a))

def _persona_match_score(pr: "PredictRequest", ps: Persona) -> float:
    cat = 1.0 if (pr.category or "").strip().lower() in [c.lower() for c in ps.categories] else 0.0
    age = _overlap_ratio(set(map(str.lower, pr.age_groups or [])), set(map(str.lower, ps.ages)))
    gen = _overlap_ratio(set(map(str.lower, pr.genders or [])), set(map(str.lower, ps.genders)))
    ints = 0.0
    if pr.interests:
        rq = set(map(str.lower, [x.strip() for x in pr.interests.split(",") if x.strip()]))
        ints = _overlap_ratio(rq, set(map(str.lower, ps.interests)))
    return 0.4*cat + 0.25*age + 0.25*gen + 0.10*ints

def sample_personas(pr: "PredictRequest", topN: int = 5) -> list[Persona]:
    scored = [(p, _persona_match_score(pr, p)) for p in PERSONAS]
    scored.sort(key=lambda x: x[1], reverse=True)
    return [p for p,_ in scored[:topN]]

# === í˜ë¥´ì†Œë‚˜ LLM ìŠ¤ì½”ì–´ë§(1~5) ë°°ì¹˜ ===
def _build_persona_scoring_prompt(pr: "PredictRequest", personas: list[Persona]) -> str:
    persona_blocks = []
    for p in personas:
        persona_blocks.append(
            f"- id:{p.id}, name:{p.name}, weight:{p.weight}, "
            f"age:{'/'.join(p.ages)}, gender:{'/'.join(p.genders)}, "
            f"interests:{'/'.join(p.interests)}, categories:{'/'.join(p.categories)}"
        )
    return f"""
ë„ˆëŠ” 30ë…„ ê²½ë ¥ì˜ í˜ë¥´ì†Œë‚˜ ë§ˆì¼€íŒ… ë¶„ì„ê°€ì•¼.
ê° í˜ë¥´ì†Œë‚˜ ì…ì¥ì—ì„œ ì•„ë˜ ë‘ ì¹´í”¼(A/B)ì— ëŒ€í•œ ì‹¬ë„ ê¹Šì€ ë¶„ì„ì„ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ë˜, ê° ë¶„ì„ ë‚´ìš©ì„ 300ì ì´ìƒì˜ ìƒì„¸í•œ ë³´ê³ ì„œ í˜•íƒœë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

[ì…ë ¥ íƒ€ê²Ÿ]
- category: {pr.category}
- ages: {", ".join(pr.age_groups or [])}
- genders: {", ".join(pr.genders or [])}
- interests: {pr.interests}

[ì¹´í”¼]
- A: {pr.marketing_a}
- B: {pr.marketing_b}

[í˜ë¥´ì†Œë‚˜]
{chr(10).join(persona_blocks)}

[ë°˜í™˜ í˜•ì‹: STRICT JSON with detailed analysis]
{{
  "results": [
    {{
      "persona_id": "p1",
      "score_a": 1,
      "score_b": 5,
      "detailed_analysis": {{
        "a_analysis": "Aì•ˆì— ëŒ€í•œ 300ì ì´ìƒì˜ ìƒì„¸í•œ ë¶„ì„ ë‚´ìš©. í˜ë¥´ì†Œë‚˜ íŠ¹ì„±ê³¼ì˜ ì—°ê´€ì„±, í´ë¦­ ì˜í–¥ì— ë¯¸ì¹˜ëŠ” ìš”ì†Œë“¤ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ìš”.",
        "b_analysis": "Bì•ˆì— ëŒ€í•œ 300ì ì´ìƒì˜ ìƒì„¸í•œ ë¶„ì„ ë‚´ìš©. í˜ë¥´ì†Œë‚˜ íŠ¹ì„±ê³¼ì˜ ì—°ê´€ì„±, í´ë¦­ ì˜í–¥ì— ë¯¸ì¹˜ëŠ” ìš”ì†Œë“¤ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ìš”.",
        "overall_evaluation": "ì „ì²´ì ì¸ ë§ˆì¼€íŒ… íš¨ê³¼ì™€ ê°œì„  ë°©í–¥, í˜ë¥´ì†Œë‚˜ë³„ ë§ì¶¤ ì „ëµì— ëŒ€í•œ 300ì ì´ìƒì˜ ì¢…í•© í‰ê°€ë¥¼ ì œê³µí•´ìš”."
      }},
      "reasons": ["í•µì‹¬ìš”ì¸1", "í•µì‹¬ìš”ì¸2", "í•µì‹¬ìš”ì¸3"]
    }}
  ],
  "winner_reason_keywords": ["í•µì‹¬1", "í•µì‹¬2", "í•µì‹¬3"]
}}

ê° í˜ë¥´ì†Œë‚˜ë³„ë¡œ detailed_analysisì˜ ê° í•­ëª©ì„ 300ì ì´ìƒì˜ ì‹¬ë„ ê¹Šì€ ë‚´ìš©ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ëª¨ë“  ë¬¸ì¥ì€ ë°˜ë“œì‹œ '-ìš”'ë¡œ ëë‚˜ì•¼ í•´ìš”.
""".strip()

def llm_persona_scores(pr: "PredictRequest", personas: list[Persona]) -> tuple[list[dict], list[str]]:
    if not llm:
        raise Exception("LangChain LLM not initialized")
    
    try:
        # í˜ë¥´ì†Œë‚˜ ì •ë³´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        persona_blocks = []
        for p in personas:
            persona_blocks.append(
                f"- id:{p.id}, name:{p.name}, weight:{p.weight}, "
                f"age:{'/'.join(p.ages)}, gender:{'/'.join(p.genders)}, "
                f"interests:{'/'.join(p.interests)}, categories:{'/'.join(p.categories)}"
            )
        
        # LangChain ì²´ì¸ ì‹¤í–‰
        chain = PERSONA_ANALYSIS_TEMPLATE | llm | StrOutputParser()
        
        result = chain.invoke({
            "category": pr.category or "",
            "ages": ", ".join(pr.age_groups or []),
            "genders": ", ".join(pr.genders or []),
            "interests": pr.interests or "",
            "marketing_a": pr.marketing_a,
            "marketing_b": pr.marketing_b,
            "personas": "\n".join(persona_blocks)
        })
        
        # JSON íŒŒì‹±
        try:
            data = json.loads(result)
        except:
            l = result.find("{"); r = result.rfind("}")
            if l >= 0 and r > l:
                data = json.loads(result[l:r+1])
            else:
                raise Exception("LLM JSON parse failed")
        
        rows = data.get("results", [])
        kw = data.get("winner_reason_keywords", [])
        
        # id â†’ weight ë§¤í•‘
        wmap = {p.id: p.weight for p in personas}
        clean = []
        
        for r in rows:
            pid = r.get("persona_id", "")
            if pid not in wmap: 
                continue
            sa = max(1, min(5, int(r.get("score_a", 3))))
            sb = max(1, min(5, int(r.get("score_b", 3))))
            rs = (r.get("reasons") or [])[:3]
            clean.append({"persona_id": pid, "w": wmap[pid], "sa": sa, "sb": sb, "reasons": rs})
        
        return clean, kw
        
    except Exception as e:
        print(f"LangChain persona scoring failed: {e}")
        raise

def weighted_ctr_from_scores(rows: list[dict]):
    if not rows:
        return 0.5, 0.5, [], []
    wsum = sum(r["w"] for r in rows) or 1.0
    a = sum(r["w"]*r["sa"] for r in rows) / wsum
    b = sum(r["w"]*r["sb"] for r in rows) / wsum
    ctr_a = a/5.0
    ctr_b = b/5.0
    ra, rb = [], []
    for r in rows:
        if r["sa"] > r["sb"]: ra += r["reasons"]
        elif r["sb"] > r["sa"]: rb += r["reasons"]
    from collections import Counter
    top = lambda xs: [w for w,_ in Counter(xs).most_common(5)]
    return ctr_a, ctr_b, top(ra), top(rb)

# === ì œ3ë¬¸êµ¬ ìƒì„±(ê·œì¹™ ì¤€ìˆ˜) ===
FORBIDDEN = ["ë¬´ë£Œì¦ì •","100% í™˜ë¶ˆ","ì „ì•¡ë³´ì¥"]
CTA_LIST  = ["ì§€ê¸ˆ ë°”ë¡œ í™•ì¸í•˜ì„¸ìš”","ì§€ê¸ˆ ì‹œì‘í•´ë³´ì„¸ìš”","ì˜¤ëŠ˜ë§Œ í˜œíƒì„ ë°›ì•„ë³´ì„¸ìš”"]

def _build_third_copy_prompt(pr: "PredictRequest", winner_keywords: list[str], winner: str) -> str:
    return f"""
ì•„ë˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” í•œêµ­ì–´ ë§ˆì¼€íŒ… í—¤ë“œë¼ì¸ì„ 1ê°œ ìƒì„±í•˜ë˜, ìƒì„± ê³¼ì •ê³¼ ê·¼ê±°ë¥¼ 300ì ì´ìƒì˜ ìƒì„¸í•œ ë¶„ì„ê³¼ í•¨ê»˜ ì œê³µí•´ì£¼ì„¸ìš”. STRICT JSONìœ¼ë¡œë§Œ ë°˜í™˜í•©ë‹ˆë‹¤. ëª¨ë“  ë¬¸ì¥ì€ ë°˜ë“œì‹œ '-ìš”'ë¡œ ëë‚˜ì•¼ í•´ìš”.

[ëª©í‘œ]
- ìŠ¹ì ì¹´í”¼({winner})ì˜ ê°•ì ì„ ë°˜ì˜í•´ ë” ë†’ì€ CTRì´ ì˜ˆìƒë˜ëŠ” ë¬¸êµ¬ 1ê°œ

[ìŠ¹ì ìš”ì¸ í‚¤ì›Œë“œ]
{", ".join(winner_keywords)}

[ì œì•½]
- 35ìì—ì„œ 45ì ì‚¬ì´
- CTA ë°˜ë“œì‹œ 1ê°œ í¬í•¨(ì˜ˆ: {", ".join(CTA_LIST)})
- ê¸ˆì¹™ì–´ í¬í•¨ ê¸ˆì§€: {", ".join(FORBIDDEN)}
- ê³¼ì¥/í—ˆìœ„ ë¶ˆê°€, ëª…í™•í•˜ë©´ì„œ ì°½ì˜ì„±ìˆê²Œ

[ë°˜í™˜ í˜•ì‹]
{{
  "text": "ìµœì¢… ë¬¸êµ¬",
  "detailed_analysis": {{
    "creation_process": "300ì ì´ìƒì˜ ë¬¸êµ¬ ìƒì„± ê³¼ì •ê³¼ ì „ëµì  ì‚¬ê³  ê³¼ì •ì„ ìƒì„¸íˆ ì„¤ëª…í•´ìš”.",
    "winner_integration": "300ì ì´ìƒì˜ ìŠ¹ì ì¹´í”¼ ê°•ì  í†µí•© ë°©ë²•ê³¼ íš¨ê³¼ ë¶„ì„ì„ ì œê³µí•´ìš”.",
    "ctr_optimization": "300ì ì´ìƒì˜ CTR í–¥ìƒì„ ìœ„í•œ í•µì‹¬ ìš”ì†Œì™€ ê°œì„  ë°©í–¥ ë¶„ì„ì„ ì œì‹œí•´ìš”.",
    "target_audience_consideration": "300ì ì´ìƒì˜ íƒ€ê²Ÿ ì˜¤ë””ì–¸ìŠ¤ íŠ¹ì„±ì„ ë°˜ì˜í•œ ë¬¸êµ¬ ì„¤ê³„ ê·¼ê±°ë¥¼ ì„¤ëª…í•´ìš”."
  }}
}}

ê° ë¶„ì„ í•­ëª©ì„ 300ì ì´ìƒì˜ ì‹¬ë„ ê¹Šì€ ë‚´ìš©ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. ëª¨ë“  ë¬¸ì¥ì€ ë°˜ë“œì‹œ '-ìš”'ë¡œ ëë‚˜ì•¼ í•´ìš”.
""".strip()

def _violates_rules(text: str) -> bool:
    t = (text or "").strip()
    if any(bad in t for bad in FORBIDDEN): return True
    if len(t) > 28: return True
    if not any(cta in t for cta in CTA_LIST): return True
    return False

def generate_third_copy(pr: "PredictRequest", winner_keywords: list[str], winner: str) -> str:
    if not llm:
        raise Exception("LangChain LLM not initialized")
    
    try:
        # LangChain ì²´ì¸ ì‹¤í–‰
        chain = THIRD_COPY_TEMPLATE | llm | StrOutputParser()
        
        result = chain.invoke({
            "winner": winner,
            "winner_keywords": ", ".join(winner_keywords),
            "cta_examples": ", ".join(CTA_LIST),
            "forbidden_words": ", ".join(FORBIDDEN)
        })
        
        # JSON íŒŒì‹±
        try:
            data = json.loads(result)
        except:
            l = result.find("{"); r = result.rfind("}")
            data = json.loads(result[l:r+1]) if l >= 0 and r > l else {"text": ""}
        
        text = (data.get("text") or "").strip()
        
        # ê·œì¹™ ê²€ì¦ ë° ìˆ˜ì •
        if _violates_rules(text):
            if not any(cta in text for cta in CTA_LIST):
                text = (text[:max(0, 28-len(CTA_LIST[0])-1)] + " " + CTA_LIST[0]).strip()
            text = text[:28]
            for bad in FORBIDDEN:
                text = text.replace(bad, "")
            text = text.strip()
        
        return text
        
    except Exception as e:
        print(f"LangChain third copy generation failed: {e}")
        # Fallback: ê¸°ë³¸ ë¬¸êµ¬ ìƒì„±
        return f"AIê°€ ìƒì„±í•œ ë§ˆì¼€íŒ… ë¬¸êµ¬: {pr.marketing_a}ì™€ {pr.marketing_b}ì˜ ì¥ì ì„ ê²°í•©í•œ ìƒˆë¡œìš´ ì ‘ê·¼"

# === CTR ìº˜ë¦¬ë¸Œë ˆì´ì…˜ (í˜„ì‹¤ ë²”ìœ„ë¡œ ë§¤í•‘) ===
CALIBRATION = {
    # ì¹´í…Œê³ ë¦¬ë³„(ì˜ˆì‹œ) í˜„ì‹¤ ë²”ìœ„: ìµœì†Œ~ìµœëŒ€ CTR (ë¹„ìœ¨)
    "default":   {"min": 0.003, "max": 0.050, "prior_mu": 0.012},  # 0.3% ~ 5.0%, í‰ê·  1.2%
    
    # PERSONAS ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ë³„ CTR ë²”ìœ„
    "beauty":    {"min": 0.004, "max": 0.055, "prior_mu": 0.017},  # ë·°í‹°/í™”ì¥í’ˆ
    "cosmetics": {"min": 0.004, "max": 0.055, "prior_mu": 0.017},  # ë·°í‹°/í™”ì¥í’ˆ
    "skincare":  {"min": 0.004, "max": 0.055, "prior_mu": 0.017},  # ë·°í‹°/í™”ì¥í’ˆ
    
    "gaming":    {"min": 0.003, "max": 0.040, "prior_mu": 0.010},  # ê²Œì„
    "electronics":{"min": 0.003, "max": 0.040, "prior_mu": 0.010},  # ê²Œì„
    "entertainment":{"min": 0.003, "max": 0.040, "prior_mu": 0.010}, # ê²Œì„
    
    "fashion":   {"min": 0.004, "max": 0.060, "prior_mu": 0.018},  # íŒ¨ì…˜/ì¡í™”
    "accessories":{"min": 0.004, "max": 0.060, "prior_mu": 0.018},  # íŒ¨ì…˜/ì¡í™”
    "lifestyle": {"min": 0.004, "max": 0.050, "prior_mu": 0.015},  # íŒ¨ì…˜/ì¡í™”
    
    "real_estate":{"min": 0.002, "max": 0.030, "prior_mu": 0.008}, # ë¶€ë™ì‚°/ì¬í…Œí¬
    "investment": {"min": 0.002, "max": 0.030, "prior_mu": 0.008}, # ë¶€ë™ì‚°/ì¬í…Œí¬
    "finance":   {"min": 0.002, "max": 0.030, "prior_mu": 0.008},  # ë¶€ë™ì‚°/ì¬í…Œí¬
    
    "travel":    {"min": 0.005, "max": 0.065, "prior_mu": 0.022},  # ì—¬í–‰/ìˆ™ë°•/í•­ê³µ
    "accommodation":{"min": 0.005, "max": 0.065, "prior_mu": 0.022}, # ì—¬í–‰/ìˆ™ë°•/í•­ê³µ
    "aviation": {"min": 0.005, "max": 0.065, "prior_mu": 0.022},  # ì—¬í–‰/ìˆ™ë°•/í•­ê³µ
    
    "sports":    {"min": 0.003, "max": 0.045, "prior_mu": 0.012},  # ìŠ¤í¬ì¸ /ë ˆì €
    "outdoor":   {"min": 0.003, "max": 0.045, "prior_mu": 0.012},  # ìŠ¤í¬ì¸ /ë ˆì €
    "leisure":   {"min": 0.003, "max": 0.045, "prior_mu": 0.012},  # ìŠ¤í¬ì¸ /ë ˆì €
    
    "food":      {"min": 0.005, "max": 0.060, "prior_mu": 0.020},  # ì‹ìŒë£Œ/ìš”ë¦¬
    "beverage":  {"min": 0.005, "max": 0.060, "prior_mu": 0.020},  # ì‹ìŒë£Œ/ìš”ë¦¬
    "cooking":   {"min": 0.005, "max": 0.060, "prior_mu": 0.020},  # ì‹ìŒë£Œ/ìš”ë¦¬
    
    "politics":  {"min": 0.001, "max": 0.020, "prior_mu": 0.005},  # ì •ì¹˜/ì‚¬íšŒ
    "social_issues":{"min": 0.001, "max": 0.020, "prior_mu": 0.005}, # ì •ì¹˜/ì‚¬íšŒ
    "news":      {"min": 0.001, "max": 0.020, "prior_mu": 0.005},  # ì •ì¹˜/ì‚¬íšŒ
    
    # ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ë“¤ (í˜¸í™˜ì„± ìœ ì§€)
    "sportswear":{"min": 0.004, "max": 0.050, "prior_mu": 0.015},
    "home":      {"min": 0.003, "max": 0.040, "prior_mu": 0.011},
}

def _calib_entry(cat: Optional[str]):
    if not cat:
        return CALIBRATION["default"]
    key = (cat or "").lower().strip()
    return CALIBRATION.get(key, CALIBRATION["default"])

def calibrate_ctr(raw_score: float, category: Optional[str], shrink: float = 0.35) -> float:
    """
    raw_score: 0~1 ìƒëŒ€ ìŠ¤ì½”ì–´
    1) ì¹´í…Œê³ ë¦¬ë³„ í˜„ì‹¤ ë²”ìœ„ [min,max]ë¡œ ì„ í˜• ë§¤í•‘
    2) prior í‰ê· (prior_mu)ë¡œ shrink (ê°€ì¤‘ í‰ê· )
    """
    e = _calib_entry(category)
    s = max(0.0, min(1.0, float(raw_score)))
    mapped = e["min"] + s * (e["max"] - e["min"])
    calibrated = (1.0 - shrink) * mapped + shrink * e["prior_mu"]
    return float(max(e["min"], min(e["max"], calibrated)))

# === LangChain ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ===
class MarketingAnalysisTool(BaseTool):
    name: str = "marketing_analysis"
    description: str = "ë§ˆì¼€íŒ… ë¬¸êµ¬ì˜ CTRì„ ì˜ˆì¸¡í•˜ê³  ìƒì„¸ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤"
    
    def _run(self, marketing_a: str, marketing_b: str, category: str, target_info: str) -> str:
        """ë§ˆì¼€íŒ… ë¶„ì„ ë„êµ¬ ì‹¤í–‰"""
        try:
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„
            keywords_a = set(marketing_a.lower().split())
            keywords_b = set(marketing_b.lower().split())
            
            # ì¹´í…Œê³ ë¦¬ ê´€ë ¨ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
            category_keywords = {
                "beauty": ["ë·°í‹°", "í™”ì¥í’ˆ", "ìŠ¤í‚¨ì¼€ì–´", "ë¯¸ìš©"],
                "fashion": ["íŒ¨ì…˜", "ì˜ë¥˜", "ìŠ¤íƒ€ì¼", "íŠ¸ë Œë“œ"],
                "electronics": ["ì „ì", "ê¸°ìˆ ", "ë””ì§€í„¸", "ìŠ¤ë§ˆíŠ¸"],
                "food": ["ìŒì‹", "ë§›", "ìš”ë¦¬", "ì‹ì‚¬"]
            }
            
            cat_keywords = category_keywords.get(category.lower(), [])
            
            # ê°„ë‹¨í•œ ì ìˆ˜ ê³„ì‚°
            score_a = sum(1 for word in keywords_a if word in cat_keywords)
            score_b = sum(1 for word in keywords_b if word in cat_keywords)
            
            return f"Aì•ˆ ì ìˆ˜: {score_a}, Bì•ˆ ì ìˆ˜: {score_b}, ì¹´í…Œê³ ë¦¬: {category}"
            
        except Exception as e:
            return f"ë¶„ì„ ì‹¤íŒ¨: {str(e)}"

# ë§ˆì¼€íŒ… ë¶„ì„ ì—ì´ì „íŠ¸ ìƒì„±
def create_marketing_agent():
    """ë§ˆì¼€íŒ… ë¶„ì„ì„ ìœ„í•œ LangChain ì—ì´ì „íŠ¸ ìƒì„±"""
    if not llm:
        return None
    
    tools = [MarketingAnalysisTool()]
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ë§ˆì¼€íŒ… ë¶„ì„ê°€ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ìš”ì²­ì— ë”°ë¼ ë§ˆì¼€íŒ… ë¬¸êµ¬ë¥¼ ë¶„ì„í•˜ê³  ìµœì í™” ë°©ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”.

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {tools}
ë„êµ¬ ì´ë¦„: {tool_names}"""),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"),
    ])
    
    agent = create_structured_chat_agent(llm, tools, prompt)
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
    
    return agent_executor

# ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
marketing_agent = create_marketing_agent()

# === ê³¼ê±° 'ìµœì¢…ì„ íƒ' ë ˆì½”ë“œ ì¸ë±ìŠ¤ ===
_INDEX = []  # {log_id, vec, ts, category, ages:set, genders:set, A, B, final_text, final_class}

def _to_class(rec) -> str:
    ft = (rec.get("user_final_text") or "").strip()
    if not ft: return ""
    if ft == (rec.get("marketing_a") or ""): return "A"
    if ft == (rec.get("marketing_b") or ""): return "B"
    return "C"

def _load_jsonl(path: Path):
    if not path.exists(): return []
    out=[]
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            line=line.strip()
            if not line: continue
            try: out.append(json.loads(line))
            except: pass
    return out

def _rebuild_index():
    global _INDEX
    _INDEX = []
    for r in _load_jsonl(RESULTS_PATH):
        if "pred_ctr_a" not in r or "pred_ctr_b" not in r: 
            continue
        c = _to_class(r)
        if not c: 
            continue
        feat = _feature_sentence(
            r.get("category",""), r.get("age_groups",[]), r.get("genders",[]),
            r.get("interests",""), r.get("marketing_a",""), r.get("marketing_b","")
        )
        try:
            vec = _embed_text(feat)
        except Exception:
            continue
        _INDEX.append({
            "log_id": r.get("log_id"),
            "vec": vec,
            "ts": float(r.get("timestamp", time.time())),
            "category": (r.get("category","") or "").lower().strip(),
            "ages": set(map(str.lower, r.get("age_groups",[]))),
            "genders": set(map(str.lower, r.get("genders",[]))),
            "A": r.get("marketing_a",""),
            "B": r.get("marketing_b",""),
            "final_text": r.get("user_final_text",""),
            "final_class": c,
        })

def _recency_weight(ts: float, alpha: float = 0.03) -> float:
    days = max(0.0, (time.time() - ts) / 86400.0)
    return 1.0 / (1.0 + alpha * days)

def _knn_follow(category:str, ages:list[str], genders:list[str], interests:str, A:str, B:str, 
                k:int=8, tau_hard:float=0.92):
    if not _INDEX:
        return {"mode":"none"}
    feat = _feature_sentence(category, ages or [], genders or [], interests or "", A or "", B or "")
    try:
        q = _embed_text(feat)
    except Exception:
        return {"mode":"none"}

    sims = []
    for item in _INDEX:
        sim = _cosine(q, item["vec"])
        sims.append((sim, item))
    sims.sort(key=lambda x: x[0], reverse=True)
    top = sims[:k]
    if not top: return {"mode":"none"}

    top1_sim, top1_item = top[0]
    if top1_sim >= tau_hard:
        return {"mode":"hard", "follow_class": top1_item["final_class"], "sim": top1_sim}

    total_w=0.0
    votes={"A":0.0,"B":0.0,"C":0.0}
    for sim, item in top:
        cat_boost = 1.15 if item["category"] == (category or "").lower().strip() else 1.0
        w = (0.7*sim + 0.3*_recency_weight(item["ts"])) * cat_boost
        votes[item["final_class"]] += w
        total_w += w
    if total_w <= 0: return {"mode":"none"}
    for k_ in votes: votes[k_] /= total_w
    return {"mode":"soft", "probs": votes, "top_sim": top1_sim}


# -----------------------------
# í™˜ê²½ ë¡œë“œ ë° ì•±
# -----------------------------
_ENV_PATH = Path(__file__).resolve().parents[1] / ".env"
if _ENV_PATH.exists():
    load_dotenv(dotenv_path=_ENV_PATH)

app = FastAPI(title="ab-test-backend")

# CORS for frontend (local development)
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://localhost:3001",
        "http://localhost:5173",
        "http://localhost:8080",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì„œë²„ ê¸°ë™ ì‹œ ì¸ë±ìŠ¤ êµ¬ì„±(í‚¤ ì—†ìœ¼ë©´ ìŠ¤í‚µ)
try:
    _rebuild_index()
except Exception as e:
    print(f"[WARN] follow-index build skipped: {e}")


# -----------------------------
# ìŠ¤í‚¤ë§ˆ
# -----------------------------
class PredictRequest(BaseModel):
    age_groups: List[str] = Field(default_factory=list)
    genders: List[str] = Field(default_factory=list)
    interests: Optional[str] = None
    category: Optional[str] = None
    marketing_a: str = Field("", description="Text for option A")
    marketing_b: str = Field("", description="Text for option B")

class PredictResponse(BaseModel):
    ctr_a: float
    ctr_b: float
    ctr_c: float
    analysis_a: str
    analysis_b: str
    analysis_c: str
    ai_suggestion: str
    # ì‘ë‹µ ë³´ê°• í•„ë“œ
    ai_top_ctr_choice: Optional[str] = None
    log_id: Optional[str] = None

class ImageGenerationRequest(BaseModel):
    marketing_text: str
    product_category: Optional[str] = None
    target_audience: Optional[str] = None

class ImageGenerationResponse(BaseModel):
    image_url: str
    prompt: str

# -----------------------------
# ê¸°ë³¸ ë¼ìš°íŠ¸/í—¬ìŠ¤ì²´í¬
# -----------------------------
@app.get("/")
def root():
    return {"ok": True, "service": "ab-test-backend"}

@app.get("/api/health")
def health():
    return {"ok": True, "service": "ab-test-backend"}



@app.get("/api/debug-env")
def debug_environment():
    """í™˜ê²½ ë³€ìˆ˜ ë””ë²„ê¹…ì„ ìœ„í•œ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "gemini_api_key_exists": bool(os.getenv("GEMINI_API_KEY")),
        "gemini_api_key_prefix": os.getenv("GEMINI_API_KEY", "")[:8] + "..." if os.getenv("GEMINI_API_KEY") else "None",
        "gemini_model": os.getenv("GEMINI_MODEL", "gemini-1.5-flash"),
        "gemini_temperature": os.getenv("GEMINI_TEMPERATURE", "0.2"),
        "env_file_exists": os.path.exists(".env"),
        "current_working_dir": os.getcwd(),
        "python_path": os.environ.get("PYTHONPATH", "Not set")
    }

# -----------------------------
# LLM í”„ë¡¬í”„íŠ¸/í˜¸ì¶œ (ì˜ˆë¹„; í˜„ì¬ predictëŠ” í˜ë¥´ì†Œë‚˜ ë°©ì‹ ì‚¬ìš©)
# -----------------------------
def _build_prompt(payload: PredictRequest) -> str:
    audience = ", ".join(filter(None, [
        "/".join(payload.age_groups) if payload.age_groups else "",
        "/".join(payload.genders) if payload.genders else "",
        payload.interests or ""
    ])).strip(", ")

    prompt = f"""
í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ë˜, ëª¨ë“  ë¬¸ì¥ì„ '-ìš”'ë¡œ ëë‚˜ê²Œ ì‘ì„±í•´ìš”. ë‘ ê´‘ê³  ë¬¸êµ¬ A/Bì˜ CTRì„ ì˜ˆì¸¡í•˜ê³  ê°ê°ì— ëŒ€í•´ 300ì ì´ìƒì˜ ì‹¬ë„ ê¹Šì€ ë¶„ì„ì„ ì œê³µí•´ìš”. ì‹œì¥ì§€ì‹, ëª…í™•ì„±, ê´€ë ¨ì„±, êµ¬ì²´ì„±, í›„í‚¹(ìˆ˜ì¹˜, ê¸´ë°•ê°, ì‚¬íšŒì  ì¦ê±°)ì„ ê·¼ê±°ë¡œ í‰ê°€í•´ìš”.

íƒ€ê²Ÿ: {audience or 'N/A'}
ì œí’ˆ/ì¹´í…Œê³ ë¦¬: {payload.category or 'N/A'}

Aì•ˆ: {payload.marketing_a}
Bì•ˆ: {payload.marketing_b}

ë°˜ë“œì‹œ ë‹¤ìŒ í‚¤ë¥¼ ê°€ì§„ JSONë§Œ ì¶œë ¥í•´ìš”: ctr_a, ctr_b, analysis_a, analysis_b, ai_suggestion.
ê·œì¹™:
- ctr_a/ctr_bëŠ” 0~1 ì‚¬ì´ ì‹¤ìˆ˜ì˜ˆìš” (ì˜ˆ: 0.123ì€ 12.3%).
- analysis_a/analysis_bëŠ” ê°ê° 300ì ì´ìƒì˜ ì‹¬ë„ ê¹Šì€ ë¶„ì„ì´ í¬í•¨ëœ ë¬¸ì¥í˜• í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ˆìš”. JSON/ì½”ë“œ/ë§ˆí¬ë‹¤ìš´ ê¸ˆì§€.
- ai_suggestionì€ ì´ íƒ€ê²Ÿì— ë§ì¶˜ í•œ ì¤„ ë¬¸êµ¬ì˜ˆìš”.
- ëª¨ë“  ë¬¸ì¥ì€ ë°˜ë“œì‹œ '-ìš”'ë¡œ ëë‚˜ì•¼ í•´ìš”.

ê° ë¶„ì„ì€ ë‹¤ìŒ ìš”ì†Œë“¤ì„ í¬í•¨í•˜ì—¬ 300ì ì´ìƒìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:
- ë§ˆì¼€íŒ… ë¬¸êµ¬ì˜ ê°•ì ê³¼ ì•½ì  ë¶„ì„
- íƒ€ê²Ÿ ì˜¤ë””ì–¸ìŠ¤ì™€ì˜ ì—°ê´€ì„± í‰ê°€
- CTR ì˜ˆì¸¡ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” í•µì‹¬ ìš”ì†Œ ë¶„ì„
- ê°œì„  ë°©í–¥ê³¼ ìµœì í™” ì „ëµ ì œì‹œ
""".strip()
    return prompt

def _call_llm(prompt: str) -> PredictResponse:
    if not llm:
        raise HTTPException(status_code=500, detail="LangChain LLM not initialized")

    try:
        # LangChain ì²´ì¸ ì‹¤í–‰
        chain = COPY_ANALYSIS_TEMPLATE | llm | StrOutputParser()
        
        # í”„ë¡¬í”„íŠ¸ì—ì„œ í•„ìš”í•œ ì •ë³´ ì¶”ì¶œ
        # í”„ë¡¬í”„íŠ¸ í˜•ì‹ì— ë§ê²Œ íŒŒì‹±í•˜ì—¬ í•„ìš”í•œ ë³€ìˆ˜ë“¤ì„ ì¶”ì¶œ
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ë°©ë²•ìœ¼ë¡œ ì²˜ë¦¬í•˜ë˜, ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŒŒì‹±ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        audience = "ì¼ë°˜ ì‚¬ìš©ì"
        category = "ì¼ë°˜"
        marketing_a = "Aì•ˆ"
        marketing_b = "Bì•ˆ"
        
        # í”„ë¡¬í”„íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ ì‹œë„
        if "íƒ€ê²Ÿ:" in prompt:
            audience = prompt.split("íƒ€ê²Ÿ:")[1].split("\n")[0].strip()
        if "ì œí’ˆ/ì¹´í…Œê³ ë¦¬:" in prompt:
            category = prompt.split("ì œí’ˆ/ì¹´í…Œê³ ë¦¬:")[1].split("\n")[0].strip()
        if "Aì•ˆ:" in prompt:
            marketing_a = prompt.split("Aì•ˆ:")[1].split("\n")[0].strip()
        if "Bì•ˆ:" in prompt:
            marketing_b = prompt.split("Bì•ˆ:")[1].split("\n")[0].strip()
        
        result = chain.invoke({
            "audience": audience,
            "category": category,
            "marketing_a": marketing_a,
            "marketing_b": marketing_b
        })
        
        # JSON íŒŒì‹±
        try:
            data = json.loads(result)
        except Exception as exc:
            # Try to extract JSON substring if the model added prose
            try:
                start = result.find("{")
                end = result.rfind("}") + 1
                if start != -1 and end != -1:
                    data = json.loads(result[start:end])
                else:
                    raise
            except Exception:
                raise HTTPException(status_code=502, detail=f"LLM parsing failed: {exc}")

        # Validate required keys
        required_keys = ["ctr_a", "ctr_b", "analysis_a", "analysis_b", "ai_suggestion"]
        missing = [k for k in required_keys if k not in data]
        if missing:
            raise HTTPException(status_code=502, detail=f"LLM response missing keys: {', '.join(missing)}")

        def _cleanup_text(text: str) -> str:
            s = (text or "").strip()
            # Flatten JSON-y text
            if s.startswith("{") or s.startswith("["):
                try:
                    obj = json.loads(s)
                    if isinstance(obj, dict):
                        s = "; ".join([f"{k}: {v}" for k, v in obj.items()])
                    elif isinstance(obj, list):
                        s = ". ".join(map(str, obj))
                except Exception:
                    pass
            return s

        return PredictResponse(
            ctr_a=float(data["ctr_a"]),
            ctr_b=float(data["ctr_b"]),
            analysis_a=_cleanup_text(str(data["analysis_a"])),
            analysis_b=_cleanup_text(str(data["analysis_b"])),
            ai_suggestion=_cleanup_text(str(data["ai_suggestion"])),
        )
        
    except Exception as exc:
        print(f"LangChain LLM call failed: {exc}")
        raise HTTPException(status_code=502, detail=f"LangChain LLM error: {exc}")

def _call_llm_for_c_analysis(req: "PredictRequest", ai_text: str) -> PredictResponse:
    """Cì•ˆ ë¶„ì„ì„ ìœ„í•œ LangChain í•¨ìˆ˜"""
    if not llm:
        raise HTTPException(status_code=500, detail="LangChain LLM not initialized")
    
    try:
        # LangChain ì²´ì¸ ì‹¤í–‰
        chain = C_ANALYSIS_TEMPLATE | llm | StrOutputParser()
        
        # íƒ€ê²Ÿ ì •ë³´ êµ¬ì„±
        audience = f"{', '.join(req.age_groups or [])} {', '.join(req.genders or [])} {req.interests or ''}"
        
        result = chain.invoke({
            "audience": audience,
            "category": req.category or "",
            "ai_text": ai_text
        })
        
        # JSON íŒŒì‹±
        try:
            data = json.loads(result)
        except Exception as exc:
            # Try to extract JSON substring if the model added prose
            try:
                start = result.find("{")
                end = result.rfind("}") + 1
                if start != -1 and end != -1:
                    data = json.loads(result[start:end])
                else:
                    raise
            except Exception:
                raise HTTPException(status_code=502, detail=f"C analysis LLM parsing failed: {exc}")

        # analysis_c í‚¤ í™•ì¸
        if "analysis_c" not in data:
            raise HTTPException(status_code=502, detail="C analysis LLM response missing analysis_c key")

        # ê¸°ë³¸ PredictResponse ë°˜í™˜ (Cì•ˆ ë¶„ì„ë§Œ í¬í•¨)
        return PredictResponse(
            ctr_a=0.0,
            ctr_b=0.0,
            ctr_c=0.0,
            analysis_a="",
            analysis_b="",
            analysis_c=str(data["analysis_c"]),
            ai_suggestion=""
        )
        
    except Exception as exc:
        print(f"LangChain C analysis failed: {exc}")
        raise HTTPException(status_code=502, detail=f"LangChain C analysis error: {exc}")

# -----------------------------
# ì˜ˆì¸¡ ì—”ë“œí¬ì¸íŠ¸
# -----------------------------
@app.post("/api/predict", response_model=PredictResponse)
def predict(req: PredictRequest):
    try:
        # 1) í˜ë¥´ì†Œë‚˜ ìƒ˜í”Œë§
        personas = sample_personas(req, topN=5)

        # 2) LLM í‰ê°€(ë°°ì¹˜) â†’ ì •ê·œí™” CTR(0~1 ìƒëŒ€ ìŠ¤ì½”ì–´)
        rows, winner_kw = llm_persona_scores(req, personas)
        ctr_a, ctr_b, reasons_a, reasons_b = weighted_ctr_from_scores(rows)
    except Exception as e:
        # Gemini API í• ë‹¹ëŸ‰ ì´ˆê³¼ ë“± ì—ëŸ¬ ì‹œ ê¸°ë³¸ê°’ ì‚¬ìš©
        error_msg = str(e)
        print(f"í˜ë¥´ì†Œë‚˜ í‰ê°€ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {error_msg}")
        
        # ì—ëŸ¬ íƒ€ì…ë³„ ìƒì„¸ ì •ë³´
        if "quota" in error_msg.lower() or "rate_limits" in error_msg.lower():
            print("ğŸ’° Gemini API í• ë‹¹ëŸ‰ ì´ˆê³¼: ë¬´ë£Œ í‹°ì–´ëŠ” ë¶„ë‹¹ 2íšŒ ìš”ì²­ ì œí•œì´ ìˆìŠµë‹ˆë‹¤")
            print("   ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ìœ ë£Œ í”Œëœìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œí•˜ì„¸ìš”")
        elif "internal error" in error_msg.lower():
            print("ğŸ”§ Gemini API ë‚´ë¶€ ì˜¤ë¥˜: ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”")
        elif "api_key" in error_msg.lower():
            print("ğŸ”‘ Gemini API í‚¤ ë¬¸ì œ: .env íŒŒì¼ì— GEMINI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”")
        else:
            print(f"âŒ ê¸°íƒ€ ì—ëŸ¬: {error_msg}")
        
        ctr_a, ctr_b = 0.45, 0.55  # ê¸°ë³¸ CTR ê°’
        reasons_a, reasons_b = "ê¸°ë³¸ ë¶„ì„", "ê¸°ë³¸ ë¶„ì„"
        winner_kw = None

    # 3) ê³¼ê±° ìœ ì‚¬ ìƒí™© íŒ”ë¡œìš°(í•˜ë“œ/ì†Œí”„íŠ¸)ë¡œ ë³´ì •
    follow = _knn_follow(
        category=req.category,
        ages=req.age_groups or [],
        genders=req.genders or [],
        interests=req.interests or "",
        A=req.marketing_a, B=req.marketing_b,
        k=8, tau_hard=0.92
    )
    if follow.get("mode") == "hard":
        if follow["follow_class"] == "A":
            ctr_a, ctr_b = max(ctr_a, 0.65), min(ctr_b, 0.35)
        elif follow["follow_class"] == "B":
            ctr_b, ctr_a = max(ctr_b, 0.65), min(ctr_a, 0.35)
        else:
            ctr_a, ctr_b = 0.45, 0.45
    elif follow.get("mode") == "soft":
        probs = follow["probs"]
        lam = 0.35
        ctr_a = (1-lam)*ctr_a + lam*probs.get("A",0.0)
        ctr_b = (1-lam)*ctr_b + lam*probs.get("B",0.0)

    # 4) í˜„ì‹¤ CTR ë²”ìœ„ë¡œ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ (ë¹„ìœ¨ê°’)
    ctr_a = calibrate_ctr(ctr_a, req.category, shrink=0.35)
    ctr_b = calibrate_ctr(ctr_b, req.category, shrink=0.35)

    # 5) ìŠ¹ì ìš”ì¸ ë°˜ì˜ ì œ3ë¬¸êµ¬
    try:
        third = generate_third_copy(
            req,
            winner_kw or (reasons_a if ctr_a >= ctr_b else reasons_b),
            "A" if ctr_a >= ctr_b else "B"
        )
    except Exception as e:
        print(f"ì œ3ë¬¸êµ¬ ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
        third = f"AIê°€ ìƒì„±í•œ ë§ˆì¼€íŒ… ë¬¸êµ¬: {req.marketing_a}ì™€ {req.marketing_b}ì˜ ì¥ì ì„ ê²°í•©í•œ ìƒˆë¡œìš´ ì ‘ê·¼"

    # 6) Cì•ˆì˜ CTR ì˜ˆì¸¡ (ìŠ¹ì ìš”ì¸ì„ ë°˜ì˜í•˜ì—¬ ë†’ì€ CTR ì˜ˆìƒ)
    ctr_c = max(ctr_a, ctr_b) * 1.1  # ìŠ¹ìë³´ë‹¤ 10% ë†’ê²Œ ì˜ˆì¸¡
    ctr_c = min(ctr_c, 0.95)  # ìµœëŒ€ 95%ë¡œ ì œí•œ
    ctr_c = calibrate_ctr(ctr_c, req.category, shrink=0.35)

    # 7) ìµœê³  CTR ê²°ì •
    if ctr_c >= max(ctr_a, ctr_b):
        ai_top = "C"
    elif ctr_a >= ctr_b:
        ai_top = "A"
    else:
        ai_top = "B"

            # 8) LLMìœ¼ë¡œ ìƒì„¸í•œ ë¶„ì„ë¬¸êµ¬ ìƒì„± (Aì•ˆ, Bì•ˆ)
    detailed_analysis = None
    try:
        detailed_analysis = _call_llm(_build_prompt(req))
    except Exception as e:
        print(f"Aì•ˆ, Bì•ˆ ë¶„ì„ ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
        detailed_analysis = None
    
    # 9) Cì•ˆ ë¶„ì„ ìƒì„± (Gemini API ì‹¤íŒ¨ ì‹œ ë¡œì»¬ ë¶„ì„ ìƒì„±)
    c_analysis_response = None
    try:
        c_analysis_response = _call_llm_for_c_analysis(req, third)
    except Exception as e:
        print(f"Cì•ˆ ë¶„ì„ ìƒì„± ì‹¤íŒ¨, ë¡œì»¬ ë¶„ì„ ìƒì„±: {e}")
        # Gemini API ì‹¤íŒ¨ ì‹œ ë¡œì»¬ì—ì„œ ë¶„ì„ ìƒì„±
        c_analysis_response = _generate_local_c_analysis(req, third, ctr_c, ctr_a, ctr_b)
    
    # 10) Cì•ˆì˜ ìƒì„¸ ë¶„ì„ ìƒì„±
    if c_analysis_response and hasattr(c_analysis_response, 'analysis_c'):
        c_analysis = c_analysis_response.analysis_c
    else:
        # fallback: ë¡œì»¬ ë¶„ì„ ìƒì„±
        c_analysis = _generate_local_c_analysis_text(req, third, ctr_c, ctr_a, ctr_b)
    
    # 11) ì €ì¥(JSONL) ë° ì‘ë‹µ
    # detailed_analysisê°€ Noneì¸ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
    if detailed_analysis is None:
        analysis_a = "ê¸°ë³¸ ë¶„ì„: ë§ˆì¼€íŒ… ë¬¸êµ¬ì˜ íš¨ê³¼ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ ì¶”ê°€ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤."
        analysis_b = "ê¸°ë³¸ ë¶„ì„: ë§ˆì¼€íŒ… ë¬¸êµ¬ì˜ íš¨ê³¼ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ ì¶”ê°€ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤."
    else:
        analysis_a = detailed_analysis.analysis_a
        analysis_b = detailed_analysis.analysis_b
    
    result = PredictResponse(
        ctr_a=float(ctr_a),
        ctr_b=float(ctr_b),
        ctr_c=float(ctr_c),
        analysis_a=analysis_a,
        analysis_b=analysis_b,
        analysis_c=c_analysis,
        ai_suggestion=third,
        ai_top_ctr_choice=ai_top,
        log_id=str(uuid.uuid4()),
    )
    stored = ABTestStoredResult(
        log_id=result.log_id,
        age_groups=req.age_groups or [],
        genders=req.genders or [],
        interests=req.interests or "",
        category=req.category or "",
        marketing_a=req.marketing_a,
        marketing_b=req.marketing_b,
        pred_ctr_a=float(result.ctr_a),
        pred_ctr_b=float(result.ctr_b),
        pred_ctr_c=float(result.ctr_c),
        ai_generated_text=result.ai_suggestion,
        ai_top_ctr_choice=ai_top,
        user_final_text=None,
    )
    row = stored.model_dump() if hasattr(stored,"model_dump") else stored.dict()
    append_jsonl(RESULTS_PATH, row)
    return result

# -----------------------------
# ì´ë¯¸ì§€ ìƒì„±
# -----------------------------
class ImageGenerationRequest(BaseModel):
    prompt: str
    n: int = 1
    size: str = "1024x1024"

class ImageGenerationResponse(BaseModel):
    images: List[str]

@app.post("/api/generate-images", response_model=ImageGenerationResponse)
def generate_images(req: ImageGenerationRequest):
    try:
        # LangChainì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸ ìµœì í™”
        if not llm:
            raise Exception("LangChain LLM not initialized")
        
        # ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ìµœì í™”ë¥¼ ìœ„í•œ LangChain ì²´ì¸
        image_prompt_template = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ê´‘ê³  ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ì‘ì„±ìì…ë‹ˆë‹¤. 
ë§ˆì¼€íŒ… ë¬¸êµ¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ DALL-E 3ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”."""),
            ("human", """ë‹¤ìŒ ë§ˆì¼€íŒ… ë¬¸êµ¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

ë§ˆì¼€íŒ… ë¬¸êµ¬: {marketing_copy}

ìš”êµ¬ì‚¬í•­:
- í˜„ëŒ€ì ì´ê³  ì „ë¬¸ì ì¸ ê´‘ê³  ì´ë¯¸ì§€
- ë§ˆì¼€íŒ… ë©”ì‹œì§€ë¥¼ ë³´ì™„í•˜ëŠ” ê¹”ë”í•œ ë ˆì´ì•„ì›ƒ
- ë””ì§€í„¸ ê´‘ê³ ì— ì í•© (ì†Œì…œ ë¯¸ë””ì–´, ë””ìŠ¤í”Œë ˆì´ ê´‘ê³ )
- ë¸Œëœë“œ ì¹œí™”ì ì´ê³  ì‹œê°ì ìœ¼ë¡œ ë§¤ë ¥ì 
- ë§ˆì¼€íŒ… ë©”ì‹œì§€ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ìƒ‰ìƒê³¼ ì´ë¯¸ì§€ ì‚¬ìš©
- ì¹´í”¼ì˜ ë©”ì‹œì§€ë¥¼ ì§€ì›í•˜ëŠ” ì‹œê°ì  ìš”ì†Œì— ì§‘ì¤‘

ì˜ì–´ë¡œ ì‘ì„±í•˜ê³ , êµ¬ì²´ì ì´ê³  ëª…í™•í•˜ê²Œ í‘œí˜„í•´ì£¼ì„¸ìš”.""")
        ])
        
        # í”„ë¡¬í”„íŠ¸ ìµœì í™”
        prompt_chain = image_prompt_template | llm | StrOutputParser()
        optimized_prompt = prompt_chain.invoke({
            "marketing_copy": req.prompt
        })
        
        # Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ìƒì„±
        # GeminiëŠ” í˜„ì¬ ì´ë¯¸ì§€ ìƒì„± ê¸°ëŠ¥ì´ ì œí•œì ì´ë¯€ë¡œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì´ë¯¸ì§€ ìƒì„±
        try:
            # Gemini 2.5 Proë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ìƒì„± (í…ìŠ¤íŠ¸ ê¸°ë°˜)
            model = genai.GenerativeModel('gemini-1.5-flash')
            
            # ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            image_generation_prompt = f"""
            Create a detailed description of an advertisement image based on this marketing copy: "{req.prompt}"
            
            The image should be:
            - Modern and professional
            - Suitable for digital advertising
            - Brand-friendly and visually appealing
            - Complementary to the marketing message
            
            Please describe the image in detail, including:
            - Visual elements and composition
            - Color scheme and mood
            - Key objects and their placement
            - Overall atmosphere and style
            
            Return only the image description, no additional text.
            """
            
            # Geminië¡œ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±
            response = model.generate_content(image_generation_prompt)
            image_description = response.text
            
            # ì‹¤ì œ ì´ë¯¸ì§€ ìƒì„±ì€ Geminiì—ì„œ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì„¤ëª…ì„ ë°˜í™˜
            # ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œëŠ” ë‹¤ë¥¸ ì´ë¯¸ì§€ ìƒì„± ì„œë¹„ìŠ¤ ì‚¬ìš© ê³ ë ¤
            image_urls = [f"Generated Image Description: {image_description}"]
            
        except Exception as e:
            print(f"Gemini image generation failed: {e}")
            # Fallback: ê¸°ë³¸ ì´ë¯¸ì§€ ì„¤ëª…
            image_urls = [f"Marketing Image for: {req.prompt}"]
        
        return ImageGenerationResponse(images=image_urls)
        
    except Exception as e:
        print(f"Image generation failed: {e}")
        raise HTTPException(status_code=500, detail=f"Image generation failed: {str(e)}")

# -----------------------------
# ì‚¬ìš©ì ìµœì¢… ì„ íƒ ê¸°ë¡ (ì¸í”Œë ˆì´ìŠ¤ ì—…ë°ì´íŠ¸)
# -----------------------------
class UserChoiceIn(BaseModel):
    log_id: str
    user_final_text: str  # ì‚¬ìš©ìê°€ ê³ ë¥¸ ì‹¤ì œ í…ìŠ¤íŠ¸(A/B/ì œ3 ë¬´ì—‡ì´ë“  ë¬¸ìì—´)

@app.post("/api/log-user-choice")
def log_user_choice(payload: UserChoiceIn):
    ok = update_user_choice_inplace(payload.log_id, payload.user_final_text)

    # í˜¹ì‹œ ì›ë³¸ ë ˆì½”ë“œë¥¼ ëª» ì°¾ìœ¼ë©´ ì´ë²¤íŠ¸ ë¼ì¸ìœ¼ë¡œë¼ë„ ë‚¨ê²¨ ë‘ 
    if not ok:
        append_jsonl(RESULTS_PATH, {
            "type": "user_choice_update",
            "log_id": payload.log_id,
            "timestamp": time.time(),
            "user_final_text": payload.user_final_text,
        })
    # ì¸ë±ìŠ¤ì— ì¦‰ì‹œ ë°˜ì˜
    if ok:
        for r in _load_jsonl(RESULTS_PATH)[::-1]:  # ìµœê·¼ë¶€í„° íƒìƒ‰
            if r.get("log_id") == payload.log_id and r.get("user_final_text"):
                try:
                    vec = _embed_text(_feature_sentence(
                        r.get("category",""), r.get("age_groups",[]), r.get("genders",[]),
                        r.get("interests",""), r.get("marketing_a",""), r.get("marketing_b","")
                    ))
                except Exception:
                    vec = None
                if vec is not None:
                    _INDEX.append({
                        "log_id": r.get("log_id"),
                        "vec": vec,
                        "ts": float(r.get("timestamp", time.time())),
                        "category": (r.get("category","") or "").lower().strip(),
                        "ages": set(map(str.lower, r.get("age_groups",[]))),
                        "genders": set(map(str.lower, r.get("genders",[]))),
                        "A": r.get("marketing_a",""),
                        "B": r.get("marketing_b",""),
                        "final_text": r.get("user_final_text",""),
                        "final_class": _to_class(r),
                    })
                break

    return {"ok": True, "updated_inline": ok}

# -----------------------------
# LangChain ì—ì´ì „íŠ¸ ì—”ë“œí¬ì¸íŠ¸
# -----------------------------
class AgentRequest(BaseModel):
    query: str
    marketing_a: Optional[str] = None
    marketing_b: Optional[str] = None
    category: Optional[str] = None
    target_info: Optional[str] = None

class AgentResponse(BaseModel):
    response: str
    success: bool
    error: Optional[str] = None

@app.post("/api/agent-query", response_model=AgentResponse)
def agent_query(req: AgentRequest):
    """LangChain ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•œ ë§ˆì¼€íŒ… ë¶„ì„ ì¿¼ë¦¬"""
    try:
        if not marketing_agent:
            return AgentResponse(
                response="",
                success=False,
                error="ë§ˆì¼€íŒ… ë¶„ì„ ì—ì´ì „íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            )
        
        # ì—ì´ì „íŠ¸ ì‹¤í–‰
        result = marketing_agent.invoke({
            "input": req.query,
            "chat_history": []
        })
        
        return AgentResponse(
            response=result.get("output", "ë¶„ì„ ê²°ê³¼ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."),
            success=True
        )
        
    except Exception as e:
        print(f"Agent query failed: {e}")
        return AgentResponse(
            response="",
            success=False,
            error=f"ì—ì´ì „íŠ¸ ì¿¼ë¦¬ ì‹¤íŒ¨: {str(e)}"
        )

@app.get("/api/agent-status")
def agent_status():
    """ì—ì´ì „íŠ¸ ìƒíƒœ í™•ì¸"""
    return {
        "agent_initialized": marketing_agent is not None,
        "llm_available": llm is not None,
        "embeddings_available": embeddings is not None,
        "tools": ["marketing_analysis"] if marketing_agent else []
    }

def _generate_local_c_analysis(req: "PredictRequest", ai_text: str, ctr_c: float, ctr_a: float, ctr_b: float) -> PredictResponse:
    """Gemini API ì‹¤íŒ¨ ì‹œ ë¡œì»¬ì—ì„œ Cì•ˆ ë¶„ì„ ìƒì„±"""
    try:
        # íƒ€ê²Ÿ ì •ë³´ êµ¬ì„±
        age_groups = ", ".join(req.age_groups or [])
        genders = ", ".join(req.genders or [])
        interests = req.interests or ""
        category = req.category or "ì¼ë°˜"
        
        # ìŠ¹ì ê²°ì •
        winner = "A" if ctr_a >= ctr_b else "B"
        winner_ctr = max(ctr_a, ctr_b)
        
        # ë¡œì»¬ ë¶„ì„ í…ìŠ¤íŠ¸ ìƒì„±
        analysis_c = f"""AIê°€ ìƒì„±í•œ ë§ˆì¼€íŒ… ë¬¸êµ¬ '{ai_text}'ì— ëŒ€í•œ ìƒì„¸ ë¶„ì„ì…ë‹ˆë‹¤.

ì´ ë¬¸êµ¬ëŠ” {winner}ì•ˆì˜ í•µì‹¬ ê°•ì ì„ ë°˜ì˜í•˜ì—¬ {ctr_c:.1%}ì˜ CTRì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤. 

**íƒ€ê²Ÿ ë¶„ì„**: {age_groups} {genders} {interests}ë¥¼ ëŒ€ìƒìœ¼ë¡œ í•œ {category} ì¹´í…Œê³ ë¦¬ ë§ˆì¼€íŒ…ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

**ì „ëµì  ì¥ì **: 
- {winner}ì•ˆì˜ ì„±ê³µ ìš”ì¸ì„ í†µí•©í•˜ì—¬ ë” ë†’ì€ íš¨ê³¼ ê¸°ëŒ€
- íƒ€ê²Ÿ ì˜¤ë””ì–¸ìŠ¤ì˜ íŠ¹ì„±ì„ ê³ ë ¤í•œ ë§ì¶¤í˜• ë©”ì‹œì§€ êµ¬ì„±
- ê°ì •ì  í˜¸ì†Œë ¥ê³¼ ëª…í™•í•œ ê°€ì¹˜ ì œì•ˆì˜ ê· í˜•

**CTR ì˜ˆì¸¡ ê·¼ê±°**: ê¸°ì¡´ {winner}ì•ˆ({winner_ctr:.1%}) ëŒ€ë¹„ {ctr_c/winner_ctr:.1%}ë°° í–¥ìƒëœ ì„±ê³¼ë¥¼ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."""

        return PredictResponse(
            ctr_a=0.0,
            ctr_b=0.0,
            ctr_c=ctr_c,
            analysis_a="",
            analysis_b="",
            analysis_c=analysis_c,
            ai_suggestion=""
        )
        
    except Exception as e:
        print(f"ë¡œì»¬ Cì•ˆ ë¶„ì„ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def _generate_local_c_analysis_text(req: "PredictRequest", ai_text: str, ctr_c: float, ctr_a: float, ctr_b: float) -> str:
    """ê°„ë‹¨í•œ ë¡œì»¬ Cì•ˆ ë¶„ì„ í…ìŠ¤íŠ¸ ìƒì„±"""
    try:
        winner = "A" if ctr_a >= ctr_b else "B"
        winner_ctr = max(ctr_a, ctr_b)
        
        return f"""AIê°€ ìƒì„±í•œ ë§ˆì¼€íŒ… ë¬¸êµ¬ '{ai_text}'ì— ëŒ€í•œ ë¶„ì„ì…ë‹ˆë‹¤. 

ì´ ë¬¸êµ¬ëŠ” {winner}ì•ˆì˜ ì¥ì ì„ ê²°í•©í•˜ì—¬ {ctr_c:.1%}ì˜ CTRì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤. 

íƒ€ê²Ÿ ì˜¤ë””ì–¸ìŠ¤({', '.join(req.age_groups or [])} {', '.join(req.genders or [])} {req.interests or ''})ì™€ì˜ ë†’ì€ ì—°ê´€ì„±ê³¼ êµ¬ì²´ì ì¸ ì œì•ˆìœ¼ë¡œ íš¨ê³¼ì ì¸ ë§ˆì¼€íŒ… ì„±ê³¼ë¥¼ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

{winner}ì•ˆ({winner_ctr:.1%}) ëŒ€ë¹„ {ctr_c/winner_ctr:.1%}ë°° í–¥ìƒëœ ì„±ê³¼ë¥¼ ê¸°ëŒ€í•  ìˆ˜ ìˆì–´ìš”."""
        
    except Exception as e:
        print(f"ê°„ë‹¨í•œ ë¡œì»¬ Cì•ˆ ë¶„ì„ ìƒì„± ì‹¤íŒ¨: {e}")
        return f"AIê°€ ìƒì„±í•œ ë§ˆì¼€íŒ… ë¬¸êµ¬ '{ai_text}'ì— ëŒ€í•œ ë¶„ì„ì…ë‹ˆë‹¤. ì´ ë¬¸êµ¬ëŠ” Aì•ˆê³¼ Bì•ˆì˜ ì¥ì ì„ ê²°í•©í•˜ì—¬ {ctr_c:.1%}ì˜ CTRì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤."
